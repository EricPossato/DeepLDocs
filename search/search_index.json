{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#eric-possato","title":"Eric Possato","text":"<p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercise1/main/","title":"Exercise 1","text":"<p>Deadline and Submission</p> <p> 05.sep (friday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Data Preparation and Analysis for Neural Networks</p> <p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"exercise1/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercise1/main/#exploring-class-separability-in-2d","title":"Exploring Class Separability in 2D","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"exercise1/main/#instructions","title":"Instructions","text":"<ol> <li>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:<ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> </li> <li> <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable. </p><pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Number of samples per class\nn_samples = 100  \n\n# Class parameters: (mean, std)\nclass_params = {\n    0: {\"mean\": [2, 3], \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6], \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1], \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]}\n}\n\n# Store all samples\ndata = []\nlabels = []\n\nfor label, params in class_params.items():\n    mean = params[\"mean\"]\n    std = params[\"std\"]\n\n    # Generate Gaussian distributed samples\n    x = np.random.normal(mean[0], std[0], n_samples)\n    y = np.random.normal(mean[1], std[1], n_samples)\n\n    # Stack into dataset\n    samples = np.column_stack((x, y))\n    data.append(samples)\n    labels.extend([label] * n_samples)\n\n# Combine into full dataset\ndata = np.vstack(data)\nlabels = np.array(labels)\n\n# Put into a DataFrame\ndf = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\ndf[\"class\"] = labels\n\n# Visualization\nplt.figure(figsize=(8,6))\nfor label in class_params.keys():\n    subset = df[df[\"class\"] == label]\n    plt.scatter(subset[\"x1\"], subset[\"x2\"], label=f\"Class {label}\", alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Synthetic Gaussian Dataset (4 classes)\")\n# Save image\nplt.savefig(\"synthetic_gaussian_dataset.png\")\n</code></pre><p></p> <p></p> </li> <li> <p>Analyze and Draw Boundaries:</p> <ol> <li> <p>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</p> <ul> <li> <p>Class 0 has a spread across most of the vertical axis and and is between 0.0 and 4.0 horizontally. Slightly overlaps Class 1</p> </li> <li> <p>Class 0 has a diagonal spread, between 2 and 10 vertically as well as horizontally. Slightly overlaps Class 0 in a few points and Class 2 in fewer points.</p> </li> <li> <p>Class 2 is very concentrated and relatvely isolated from other classes.</p> </li> <li> <p>Class 3 is completely isolated, spread vertically along and the furthest in the x axis.</p> </li> </ul> </li> <li> <p>Based on your visual inspection, could a simple, linear boundary separate all classes?</p> <ul> <li>A linear boundary is capable of mostly separating all classes (with some missclassified points), but because of the overlaps between class 0 and 1, a perfect and clean linear separation would fail.</li> </ul> </li> <li> <p>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</p> <ul> <li>By adding the following code to the earlier plot, we can approximate some separations between classes <pre><code># three hand-drawn separators \nplt.plot([6.0, 0.6], [-2.0, 10.0], linestyle=\"--\", linewidth=2, color=\"k\", label=\"approx. boundary\")  # between 0 &amp; 1\nplt.plot([4.0, 12.0], [2.0, 5.0], linestyle=\"--\", linewidth=2, color=\"k\")                            # between 1 &amp; 2\nplt.plot([12.0, 12.0], [-1, 9], linestyle=\"--\", linewidth=2, color=\"k\")                             # isolate class 3\n</code></pre> </li> </ul> </li> </ol> </li> </ol>"},{"location":"exercise1/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercise1/main/#non-linearity-in-higher-dimensions","title":"Non-Linearity in Higher Dimensions","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"exercise1/main/#instructions_1","title":"Instructions","text":"<ol> <li> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p> <ul> <li> <p>Class A:</p> <p>Mean vector:</p> \\[\\mu_A = [0, 0, 0, 0, 0]\\] <p>Covariance matrix:</p> \\[ \\Sigma_A = \\begin{pmatrix} 1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\ 0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\ 0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0 \\end{pmatrix} \\] </li> <li> <p>Class B:</p> <p>Mean vector:</p> \\[\\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\\] <p>Covariance matrix:</p> \\[ \\Sigma_B = \\begin{pmatrix} 1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\ -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\ 0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5 \\end{pmatrix} \\] </li> </ul> </li> <li> <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).     <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# set experimen seed\nnp.random.seed(42)\n\n# number of samples\nn_a = 500\nn_b = 500\n\n# Parameters for Class A\nmu_A = np.array([0, 0, 0, 0, 0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\n# Parameters for Class B\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2,  0.4, 1.5, 0.6, 0.0],\n    [0.0,  0.0, 0.6, 1.5, 0.3],\n    [0.0,  0.0, 0.0, 0.3, 1.5],\n])\n\n# Generate Data\nA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=n_a)\nB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=n_b)\n\n# Combine into one dataset\nX = np.vstack([A, B])\ny = np.array([0] * n_a + [1] * n_b)  # 0 = Class A, 1 = Class B\n\n# Put into DataFrame\ndf = pd.DataFrame(X, columns=[f\"x{i+1}\" for i in range(5)])\ndf[\"class\"] = y\n\n\n# Standardize features\nscaler = StandardScaler()\nX_std = scaler.fit_transform(df[[f\"x{i+1}\" for i in range(5)]])\n\n# PCA projection to 2D\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_std)\nevr = pca.explained_variance_ratio_ \n\n# ---------- Plot ----------\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], alpha=0.6, label=\"Class B\")\nplt.xlabel(f\"PC1 ({evr[0]*100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({evr[1]*100:.1f}% var)\")\nplt.title(\"Exercise 2: PCA projection to 2D (Class A vs Class B)\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"exercise2_pca_scatter.png\", dpi=150, bbox_inches=\"tight\")\nprint(\"Saved PCA figure to exercise2_pca_scatter.png\")\nplt.show()\n</code></pre> </li> </ul> </li> <li> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.<ul> <li>Both classes have similar vertical spreads. Horizontally, class A is concentrated mostly on negative values, and Class B on positive values. They both overlap between -2 and 2 within PC1 axis</li> </ul> </li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.<ul> <li>Linear models use a hyperplane to separate the data, but when classes are intertwined like shown in the image, this fails, since a hyperplane capable of separating the classes doesn't exist. </li> </ul> </li> </ol> </li> </ol>"},{"location":"exercise1/main/#exercise-3","title":"Exercise 3","text":""},{"location":"exercise1/main/#preparing-real-world-data-for-a-neural-network","title":"Preparing Real-World Data for a Neural Network","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (<code>tanh</code>) activation function in its hidden layers.</p>"},{"location":"exercise1/main/#instructions_2","title":"Instructions","text":"<ol> <li>Get the Data: Download the Spaceship Titanic dataset from Kaggle.</li> <li>Describe the Data:<ul> <li>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).<ul> <li>The dataset comes from the Kaggle competition: Spaceship Titanic. The dataset consists of records recovered from the spaceship's damaged computer system after the ship collided with an anomaly. The objective of the competition and consequently, the data, is to be able to predict which passengers were transported to an alternate dimension by the anomaly</li> </ul> </li> <li>List the features and identify which are numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>). <pre><code>import pandas as pd\n\n# Load dataset (adjust path to your train.csv)\ndf = pd.read_csv(\"./docs/exercise1/titanic_dataset/train.csv\")\n\n\n# List all columns\nprint(\"\\nColumns in dataset:\")\nprint(df.columns.tolist())\n\n# Show data types and non-null counts\nprint(\"\\nInfo about dataset:\")\nprint(df.info())\n\n\n# Quick separation into numerical vs categorical\nnumerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_cols = df.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n\nprint(\"\\nNumerical features:\", numerical_cols)\nprint(\"Categorical features:\", categorical_cols)\n</code></pre> <pre><code>Numerical features: ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nCategorical features: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name', 'Transported']\n</code></pre></li> <li>Investigate the dataset for missing values. Which columns have them, and how many? <pre><code># Summary of missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isna().sum())\n</code></pre> <pre><code>Missing values per column:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</code></pre></li> </ul> </li> <li>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.<ul> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ul> </li> <li>Visualize the Results:<ul> <li>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) before and after scaling to show the effect of your transformation.</li> </ul> </li> </ol> <p>Methods for 3 and 4:</p> <ul> <li>Numerical features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) were imputed with the median value of each column. Using median instead of mean makes it less likely that outliers influence the results.</li> <li>Categorical features (HomePlanet, CryoSleep, Destination, VIP) were imputed with the mode (most frequent value). THis preserves the most common class for each feature</li> <li>Dropped columns: Cabin and Name were removed. Name does not offer any useful information and Cabin is very complex, not usable without heavy feature engineering, so dropped for simplicity.</li> <li>Binary variables (CryoSleep, VIP) were converted into 0/1 integers. This allows direct use as inputs in a neural network.</li> <li>Nominal categorical variables (HomePlanet, Destination) were transformed using one-hot encoding. This removes ordinal bias by making each category a separate feature.</li> <li>All numerical features were scaled to [-1, 1] using Min\u2013Max normalization. The tanh activation function outputs values in the range [-1, 1] so this process scales the inputs to the required range.</li> </ul> <p>This is the code that combines all of the mentioned methos and generates histograms for the <code>Age</code> and <code>FoodCourt</code> columns: </p><pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_csv(\"./docs/exercise1/titanic_dataset/train.csv\")\n\n# Handle Missing Data\n\n# Numerical: fill with median\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    df[col] = df[col].fillna(df[col].median())\n\n# Categorical: fill with mode\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\nfor col in cat_cols:\n    # make column string dtype\n    df[col] = df[col].astype(\"string\")\n    # fill NaN with mode and convert to string dtype\n    df[col] = df[col].fillna(df[col].mode(dropna=True)[0]).astype(str)\n\n# Drop Cabin and Name\ndf.drop([\"Cabin\", \"Name\"], axis=1, inplace=True)\n\n# Convert CryoSleep and VIP from True/False strings to 0/1 ints\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].map({\"True\": 1, \"False\": 0})\ndf[\"VIP\"] = df[\"VIP\"].map({\"True\": 1, \"False\": 0})\n\n# Encode Categorical\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].astype(int)\ndf[\"VIP\"] = df[\"VIP\"].astype(int)\n\n# One-hot encode categorical vars\ndf = pd.get_dummies(df, columns=[\"HomePlanet\", \"Destination\"], drop_first=True)\n\n# Capture copy BEFORE scaling\npre_scale_num = df[num_cols].copy()\n\n# Scale Numerical Features\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf[num_cols] = scaler.fit_transform(df[num_cols].astype(\"float64\"))\n\nprint(df.head())\n\n# histograms before vs after\nfeatures_to_show = [\"Age\", \"FoodCourt\"]  # pick any subset of num_cols\n\nfor col in features_to_show:\n    if col in pre_scale_num.columns:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=False, sharey=False)\n        ax[0].hist(pre_scale_num[col].dropna(), bins=30)\n        ax[0].set_title(f\"{col} \u2014 before scaling\")\n        ax[0].set_xlabel(col); ax[0].set_ylabel(\"count\")\n\n        ax[1].hist(df[col].dropna(), bins=30)\n        ax[1].set_title(f\"{col} \u2014 after scaling (standardized)\")\n        ax[1].set_xlabel(col); ax[1].set_ylabel(\"count\")\n\n        plt.tight_layout()\n        # Save figures\n        plt.savefig(f\"hist_{col.lower()}_before_after.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n</code></pre><p></p> <p>This are the histograms generated by this code:</p> <p> </p>"},{"location":"exercise1/main/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>The deliverable for this activity consists of a report that includes:</p> <ol> <li>A brief description of your approach to each exercise.</li> <li>The code used to generate the datasets, preprocess the data, and create the visualizations. With comments explaining each step.</li> <li>The plots and visualizations requested in each exercise.</li> <li>Your analysis and answers to the questions posed in each exercise.</li> </ol> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> <p>Exercise 1 (3 points):</p> Criteria Description 1 pt Data is generated correctly and visualized in a clear scatter plot with proper labels and colors. 2 pts The analysis of class separability is accurate, and the proposed decision boundaries are logical and well-explained in the context of what a network would learn. <p>Exercise 2 (3 points):</p> Criteria Description 1 pt Data is generated correctly using the specified multivariate parameters. 1 pt Dimensionality reduction is applied correctly, and the resulting 2D projection is clearly plotted. 1 pt The analysis correctly identifies the non-linear relationship and explains why a neural network would be a suitable model. <p>Exercise 3 (4 points):</p> Criteria Description 1 pt The data is correctly loaded, and its characteristics are accurately described. 2 pts All preprocessing steps (handling missing data, encoding, and appropriate feature scaling for <code>tanh</code>) are implemented correctly and with clear justification for a neural network context. 1 pt Visualizations effectively demonstrate the impact of the data preprocessing."},{"location":"exercise2/main/","title":"Exercise 2","text":"<p>Deadline and Submission</p> <p> 14.sep (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Understanding Perceptrons and Their Limitations</p> <p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"exercise2/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercise2/main/#data-generation-task","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:  </p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([1.5, 1.5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\) (i.e., variance of \\(0.5\\) along each dimension, no covariance).  </p> </li> <li> <p>Class 1:</p> <p>Mean = \\([5, 5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\).  </p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> <p>We can use the following code to generate and plot the points: </p><pre><code>def generate_and_plot():\n    # Parameters for Class 0\n    mean_class0 = [1.5, 1.5]\n    cov_class0 = [[0.5, 0], [0, 0.5]]\n\n    # Parameters for Class 1\n    mean_class1 = [5, 5]\n    cov_class1 = [[0.5, 0], [0, 0.5]]\n\n    # Fix random seed for reproducibility\n    np.random.seed(42)\n\n    # Generate 1000 samples per class\n    class0_samples = np.random.multivariate_normal(mean_class0, cov_class0, 1000)\n    class1_samples = np.random.multivariate_normal(mean_class1, cov_class1, 1000)\n\n    # Plot the two classes\n    plt.figure(figsize=(8, 6))\n    plt.scatter(class0_samples[:, 0], class0_samples[:, 1], \n                c='blue', alpha=0.5, label=\"Class 0\")\n    plt.scatter(class1_samples[:, 0], class1_samples[:, 1], \n                c='red', alpha=0.5, label=\"Class 1\")\n\n    plt.title(\"2D Data Points from Two Classes (Multivariate Normal)\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid(True)\n    #plt.show()\n    # Save the image as a png file\n    plt.savefig(\"exercise2_ex1_plot.png\")\n    # Return values for reuse\n    return class0_samples, class1_samples\n</code></pre><p></p> <p>The following is the resulting plot of the classes: </p>"},{"location":"exercise2/main/#perceptron-implementation-task","title":"Perceptron Implementation Task:","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.  </p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).  </li> <li>Use the perceptron learning rule: For each misclassified sample \\((x, y)\\), update \\(w = w + \u03b7 * y * x\\) and \\(b = b + \u03b7 * y\\), where \\(\u03b7\\) is the learning rate (start with \\(\u03b7=0.01\\)).  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.  </li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by \\(w\u00b7x + b = 0\\)) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> <p>The following code can be used to implment and train a perceptron:</p> <p></p><pre><code># Perceptron implementation\ndef perceptron_train(X, y, lr=0.01, max_epochs=100):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)   # weight vector\n    b = 0.0                    # bias term\n    accuracies = []\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for i in range(n_samples):\n            activation = np.dot(w, X[i]) + b\n            y_pred = 1 if activation &gt;= 0 else -1\n            if y_pred != y[i]:\n                # Update rule\n                w += lr * y[i] * X[i]\n                b += lr * y[i]\n                errors += 1\n\n        # Track accuracy\n        activation = np.dot(X, w) + b\n        predictions = np.where(activation &gt;= 0, 1, -1)\n        acc = np.mean(predictions == y)\n        accuracies.append(acc)\n\n        if errors == 0:  # converged\n            print(f\"Converged after {epoch+1} epochs.\")\n            break\n\n    return w, b, accuracies, predictions\n</code></pre> We can use that code, to train a perceptron based to the parameters specified by the exercise, and plot the relevant graphs: <pre><code># Generate data\nX, y, class0_samples, class1_samples = generate_data()\n\n# Train perceptron\nw, b, accuracies, final_predictions = perceptron_train(X, y)\n\n# Results\nprint(\"\\nFinal Results:\")\nprint(\"Weights:\", w)\nprint(\"Bias:\", b)\nprint(\"Final Accuracy:\", accuracies[-1])\n\n# Plot decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nxx = np.linspace(x_min, x_max, 100)\nyy = -(w[0] * xx + b) / w[1]  # line equation\n\nplt.figure(figsize=(8, 6))\nplt.scatter(class0_samples[:, 0], class0_samples[:, 1], c='blue', alpha=0.5, label=\"Class 0 (-1)\")\nplt.scatter(class1_samples[:, 0], class1_samples[:, 1], c='red', alpha=0.5, label=\"Class 1 (+1)\")\nplt.plot(xx, yy, 'k--', label=\"Decision Boundary\")\n\n# Misclassified points\nmisclassified = X[final_predictions != y]\nif len(misclassified) &gt; 0:\n    plt.scatter(misclassified[:, 0], misclassified[:, 1],\n                edgecolors='yellow', facecolors='none', s=80, label=\"Misclassified\")\n\nplt.title(\"Perceptron Decision Boundary\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot training accuracy\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(accuracies) + 1), accuracies, marker='o')\nplt.title(\"Training Accuracy over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0, 1.05)\nplt.grid(True)\nplt.show()\n</code></pre><p></p> <p>This is the resulting decision boundary: </p> <p>This is the training accuracy over each epoch: </p> <p>It is worth mentioning that since the data used is linearly separable, the perceptron is able to perfectly classify the points given.</p>"},{"location":"exercise2/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercise2/main/#data-generation-task_1","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([3, 3]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\) (i.e., higher variance of 1.5 along each dimension).</p> </li> <li> <p>Class 1:</p> <p>Mean = \\([4, 4]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\).  </p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> <p>Using the same method as the first exercise, while changing the parameters, we get the following data:  Notice how in this case, both classes overlap in a number of areas.</p>"},{"location":"exercise2/main/#perceptron-implementation-task_1","title":"Perceptron Implementation Task:","text":"<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.  </p> <ul> <li>Follow the same initialization, update rule, and training process.  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.  </li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> <p>Since this case is not linearly separable, and the training may vary in result, we can add random initialization of the weights and bias, this is the new perceptron training function to account for that: </p><pre><code># Perceptron implementation\ndef perceptron_train(X, y, lr=0.01, max_epochs=100, random_init=False):\n    n_samples, n_features = X.shape\n    # Initialize weights randomly or zeros\n    if random_init:\n        w = np.random.randn(n_features) * 0.01\n        b = np.random.randn() * 0.01\n    else:\n        w = np.zeros(n_features)\n        b = 0.0           # bias term\n    accuracies = []\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for i in range(n_samples):\n            activation = np.dot(w, X[i]) + b\n            y_pred = 1 if activation &gt;= 0 else -1\n            if y_pred != y[i]:\n                # Update rule\n                w += lr * y[i] * X[i]\n                b += lr * y[i]\n                errors += 1\n\n        # Track accuracy\n        activation = np.dot(X, w) + b\n        predictions = np.where(activation &gt;= 0, 1, -1)\n        acc = np.mean(predictions == y)\n        accuracies.append(acc)\n\n        if errors == 0:  # converged\n            print(f\"Converged after {epoch+1} epochs.\")\n            break \n    return w, b, accuracies, predictions\n</code></pre><p></p> <p>Accounting for this change, this is reflected in the main code as follows: </p><pre><code># Generate data\nX, y, class0_samples, class1_samples = generate_data()\n\n# Train perceptron\nall_runs_acc = []\nbest_run = None\n\nfor run in range(5):\n    w, b, accuracies, final_predictions = perceptron_train(X, y, lr=0.01, max_epochs=100, random_init=True)\n    final_acc = accuracies[-1]\n    all_runs_acc.append(final_acc)\n\n    if best_run is None or final_acc &gt; best_run[\"acc\"]:\n        best_run = {\"w\": w, \"b\": b, \"acc\": final_acc, \"acc_list\": accuracies, \"pred\": final_predictions}\n\n\n# Results\nprint(\"\\nSummary over 5 runs:\")\nprint(\"Accuracies per run:\", all_runs_acc)\nprint(\"Average accuracy:\", np.mean(all_runs_acc))\nprint(\"Best accuracy:\", best_run[\"acc\"])\nprint(\"Best weights:\", best_run[\"w\"])\nprint(\"Best bias:\", best_run[\"b\"])\n\n\n\n# Plot decision boundary of best run\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nxx = np.linspace(x_min, x_max, 100)\nyy = -(best_run[\"w\"][0] * xx + best_run[\"b\"]) / best_run[\"w\"][1]\n\nplt.figure(figsize=(8, 6))\nplt.scatter(class0_samples[:, 0], class0_samples[:, 1], \n            c='blue', alpha=0.5, label=\"Class 0 (-1)\")\nplt.scatter(class1_samples[:, 0], class1_samples[:, 1], \n            c='red', alpha=0.5, label=\"Class 1 (+1)\")\nplt.plot(xx, yy, 'k--', label=\"Decision Boundary\")\n\n# Misclassified points separated by class\nmisclassified = X[best_run[\"pred\"] != y]\nmisclassified_labels = y[best_run[\"pred\"] != y]\n\n# Class 0 misclassified (true label = -1)\nplt.scatter(misclassified[misclassified_labels == -1][:, 0],\n            misclassified[misclassified_labels == -1][:, 1],\n            c='green', alpha=0.5, label=\"Misclassified Class 0\")\n\n# Class 1 misclassified (true label = +1)\nplt.scatter(misclassified[misclassified_labels == 1][:, 0],\n            misclassified[misclassified_labels == 1][:, 1],\n            c='yellow', alpha=0.5, label=\"Misclassified Class 1\")\n\nplt.title(\"Best Perceptron Decision Boundary\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.legend()\nplt.grid(True)\n\nplt.savefig(\"exercise2_ex2_perceptron_accuracy.png\")\n</code></pre><p></p> <p>This is the decision boundary generated by this code, with highlighted missclassified points in different colors: </p> <p>Due to the classes being closer together, even overlapping in a few situations, a perceptron has difficulty in finding a good separation of the data, in the case shown by the image, the perceptron seemed to favor guessing Class 1, missclassifying most of Class 0 points.</p> <p>This can be reinforced by analyzing the accuracy throughout the epochs, that remained consistently around 50%, showing the difficulty that the perceptron while performing with non linearly separable data.</p> <p></p>"},{"location":"exercise2/main/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Usage of Toolboxes</p> <p>You may use toolboxes (e.g., NumPy) ONLY for matrix operations and calculations during this activity. All other computations, including activation functions, loss calculations, gradients, and the forward pass, MUST BE IMPLEMENTED within your Perceptron code. The use of third-party libraries for the Perceptron implementation IS STRICTLY PROHIBITED.</p> <p>Failure to comply with these instructions will result in your submission being rejected.</p> <p>The deliverable for this activity consists of a report that includes:</p> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> Criteria Description 4 pts Correctness of the perceptron implementation 2 pts Exercise 1: Data generation, training, evaluation, and analysis. 2 pts Exercise 2: Data generation, training, evaluation, and analysis. 1 pt Visualizations: Quality and clarity of plots (data distribution, decision boundary, accuracy over epochs). 1 pt Report Quality: Clarity, organization, and completeness of the report."},{"location":"exercise3/main/","title":"Exercise 3","text":"<p>Deadline and Submission</p> <p> 21.sep (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Understanding Multi-Layer Perceptrons (MLPs)</p> <p>This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs).</p>"},{"location":"exercise3/main/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps","text":"<p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): \\( L = \\frac{1}{N} (y - \\hat{y})^2 \\), where \\( \\hat{y} \\) is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li> <p>Input and output vectors:</p> <p>\\( \\mathbf{x} = [0.5, -0.2] \\)</p> <p>\\( y = 1.0 \\)</p> </li> <li> <p>Hidden layer weights:</p> <p>\\( \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\)</p> </li> <li> <p>Hidden layer biases:</p> <p>\\( \\mathbf{b}^{(1)} = [0.1, -0.2] \\)</p> </li> <li> <p>Output layer weights:</p> <p>\\( \\mathbf{W}^{(2)} = [0.5, -0.3] \\)</p> </li> <li> <p>Output layer bias:</p> <p>\\( b^{(2)} = 0.2 \\)</p> </li> <li> <p>Learning rate: \\( \\eta = 0.3 \\)</p> </li> <li> <p>Activation function: \\( \\tanh \\)</p> </li> </ul> <p>We can define all of these variables with the following code: </p><pre><code># Activation function\ndef tanh(x):\n    return np.tanh(x)\n\n# Derivative of tanh\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\n# Input vector\nx = np.array([0.5, -0.2])\n\n# Target output\ny = 1.0\n\n# Hidden layer weights and biases\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\nb1 = np.array([0.1, -0.2])\n\n# Output layer weights and bias\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\n\n# Learning rate\neta = 0.3\n</code></pre><p></p> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li> <p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: \\( \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\).</li> <li>Apply tanh to get hidden activations: \\( \\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) \\).</li> <li>Compute the output pre-activation: \\( u^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + b^{(2)} \\).</li> <li>Compute the final output: \\( \\hat{y} = \\tanh(u^{(2)}) \\).</li> </ul> <p>The forward pass operations can be done with the following code: </p><pre><code># Forward Pass\nz1 = W1.dot(x) + b1\nprint(f\"z1 = {z1}\")\n\nh1 = tanh(z1)\nprint(f\"h1 = {h1}\")\n\nu2 = W2.dot(h1) + b2\nprint(f\"u2 = {u2}\")\n\ny_hat = tanh(u2)\nprint(f\"y_hat = {y_hat}\")\n</code></pre><p></p> <p>These are the results: </p><pre><code>z1 = [ 0.27 -0.18]\nh1 = [ 0.26362484 -0.17808087]\nu2 = 0.38523667817130075\ny_hat = 0.36724656264510797\n</code></pre><p></p> </li> <li> <p>Loss Calculation:</p> <ul> <li> <p>Compute the MSE loss:</p> <p>\\( L = \\frac{1}{N} (y - \\hat{y})^2 \\).</p> </li> </ul> <p>This is what the loss calculation code looks like: </p><pre><code># Loss Calculation\nL = (y - y_hat)**2\nprint(f\"Loss L = {L}\")\n</code></pre> And the result: <pre><code>Loss L = 0.4003769124844312\n</code></pre><p></p> </li> <li> <p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with \\( \\displaystyle \\frac{\\partial L}{\\partial \\hat{y}} \\), then compute:</p> <ul> <li>\\( \\displaystyle \\frac{\\partial L}{\\partial u^{(2)}} \\) (using the tanh derivative: \\( \\displaystyle \\frac{d}{du} \\tanh(u) = 1 - \\tanh^2(u) \\)).</li> <li>Gradients for output layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial b^{(2)}} \\).</li> <li>Propagate to hidden layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\).</li> <li>Gradients for hidden layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} \\).</li> </ul> <p>Show all intermediate steps and calculations.</p> <p>This is the code to calculate each operation in the backwards pass: </p><pre><code># Backward Pass\ndL_dyhat = 2 * (y_hat - y)\nprint(f\"dL/dy_hat = {dL_dyhat}\")\n\ndL_du2 = dL_dyhat * tanh_derivative(u2)\nprint(f\"dL/du2 = {dL_du2}\")\n\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\nprint(f\"dL/dW2 = {dL_dW2}\")\nprint(f\"dL/db2 = {dL_db2}\")\n\ndL_dh1 = dL_du2 * W2\nprint(f\"dL/dh1 = {dL_dh1}\")\n\ndL_dz1 = dL_dh1 * tanh_derivative(z1)\nprint(f\"dL/dz1 = {dL_dz1}\")\n\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\nprint(f\"dL/dW1 =\\n{dL_dW1}\")\nprint(f\"dL/db1 = {dL_db1}\")\n</code></pre> And these are the results: <pre><code>dL/dy_hat = -1.265506874709784\ndL/du2 = -1.0948279147135995\ndL/dW2 = [-0.28862383  0.19496791]\ndL/db2 = -1.0948279147135995\ndL/dh1 = [-0.54741396  0.32844837]\ndL/dz1 = [-0.50936975  0.31803236]\ndL/dW1 =\n[[-0.25468488  0.10187395]\n[ 0.15901618 -0.06360647]]\ndL/db1 = [-0.50936975  0.31803236]\n</code></pre><p></p> </li> <li> <p>Parameter Update: Using the learning rate \\( \\eta = 0.1 \\), update all weights and biases via gradient descent:</p> <ul> <li>\\( \\displaystyle \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} \\)</li> <li>\\( \\displaystyle b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} \\)</li> <li>\\( \\displaystyle \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} \\)</li> <li>\\( \\displaystyle \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} \\)</li> </ul> <p>Provide the numerical values for all updated parameters. This is the code for the parameter update: </p><pre><code># Parameter Update\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(\"\\nUpdated Parameters:\")\nprint(f\"W2 = {W2_new}\")\nprint(f\"b2 = {b2_new}\")\nprint(f\"W1 =\\n{W1_new}\")\nprint(f\"b1 = {b1_new}\")\n</code></pre> And this is each updated parameter: <pre><code>Updated Parameters:\nW2 = [ 0.58658715 -0.35849037]\nb2 = 0.5284483744140799\nW1 =\n[[ 0.37640546 -0.13056219]\n[ 0.15229515  0.41908194]]\nb1 = [ 0.25281093 -0.29540971]\n</code></pre><p></p> </li> </ol> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p>"},{"location":"exercise3/main/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP","text":"<p>Using the <code>make_classification</code> function from scikit-learn (documentation), generate a synthetic dataset with the following specifications:</p> <ul> <li>Number of samples: 1000</li> <li>Number of classes: 2</li> <li>Number of clusters per class: Use the <code>n_clusters_per_class</code> parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</li> <li>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</li> </ul> <p>We can generate adequate data by using the following code (note that class 1 is usiung a random_state of 24 instead of 42 to avoid both clusters having too similar of a geometry): </p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Parameters you can experiment with\nn_samples = 500\nclass_sep = 1.5\nflip_y = 0\nrandom_state_class0 = 42\nrandom_state_class1 = 24\n\n# Generate class 0 with 1 cluster\nX0, y0 = make_classification(\n    n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],\n    class_sep=class_sep, flip_y=flip_y, random_state=random_state_class0\n)\n\n# Generate class 1 with 2 clusters\nX1, y1 = make_classification(\n    n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],\n    class_sep=class_sep, flip_y=flip_y, random_state=random_state_class1\n)\n\n# Combine\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Plot\nplt.figure(figsize=(6, 6))\nplt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0 (1 cluster)', alpha=0.6)\nplt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1 (2 clusters)', alpha=0.6)\nplt.title(\"Synthetic Dataset: 1 cluster vs 2 clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.grid(True)\n#save image\nplt.savefig(\"docs/exercise3/dataset.png\")\n#plt.show()\n</code></pre><p></p> <p>This is a plot of the data: </p> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li>Number of hidden layers (at least 1)</li> <li>Number of neurons per layer</li> <li>Activation functions (e.g., sigmoid, ReLU, tanh)</li> <li>Loss function (e.g., binary cross-entropy)</li> <li>Optimizer (e.g., gradient descent, with a chosen learning rate)</li> </ul> <p>Steps to follow:</p> <ol> <li>Generate and split the data into training (80%) and testing (20%) sets.</li> <li>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</li> <li>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</li> <li>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</li> <li>Submit your code and results, including any visualizations.</li> </ol> <p>Before going into the code for each step, these are the auxiliary functions to account for the use of the sigmoid activation function, as well as binary cross-entropy loss function: </p><pre><code># Sigmoid activation\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Derivative of sigmoid\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\n# Binary cross-entropy loss\ndef binary_cross_entropy(y, y_hat):\n    eps = 1e-9\n    return -np.mean(y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n\n# Derivative of BCE wrt output pre-activation\ndef bce_derivative(y, y_hat):\n    return (y_hat - y) / (y_hat * (1 - y_hat) + 1e-9)\n</code></pre><p></p> <p>Fist, we split the dataset between train and test: </p><pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</code></pre><p></p> <p>Then, we set the parameters for the execution of the training: </p><pre><code># Network architecture\ninput_dim = 2\nhidden_dim = 4\noutput_dim = 1\n\n# Parameter initialization\nnp.random.seed(1)\nW1 = np.random.randn(hidden_dim, input_dim) * 0.01\nb1 = np.zeros((hidden_dim, 1))\nW2 = np.random.randn(output_dim, hidden_dim) * 0.01\nb2 = np.zeros((output_dim, 1))\n\n# Training setup\neta = 0.1\nepochs = 200\ntrain_losses = []\n</code></pre><p></p> <p>With the parameters set, we can setup a loop for each epoch performing the steps similar to exercise 1, but using the new activation and loss functions. We also append all losses so they can be plot later: </p><pre><code># Training loop\nfor epoch in range(epochs):\n    # Forward Pass\n    Z1 = X_train.dot(W1.T) + b1.T\n    A1 = sigmoid(Z1)\n\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = sigmoid(Z2)\n\n    # Loss Calculation\n    loss = binary_cross_entropy(y_train, A2)\n    train_losses.append(loss)\n\n    # Backward Pass\n    dZ2 = bce_derivative(y_train, A2) * sigmoid_derivative(Z2)\n    dW2 = dZ2.T.dot(A1) / X_train.shape[0]\n    db2 = np.mean(dZ2, axis=0, keepdims=True)\n\n    dA1 = dZ2.dot(W2)\n    dZ1 = dA1 * sigmoid_derivative(Z1)\n    dW1 = dZ1.T.dot(X_train) / X_train.shape[0]\n    db1 = np.mean(dZ1, axis=0, keepdims=True)\n\n    # Parameter Update\n    W1 -= eta * dW1\n    b1 -= eta * db1.T\n    W2 -= eta * dW2\n    b2 -= eta * db2.T\n\n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> After training, we evaluate the model and calculate accuracy on the test set: <pre><code>Z1 = X_test.dot(W1.T) + b1.T\nA1 = sigmoid(Z1)\nZ2 = A1.dot(W2.T) + b2.T\nA2 = sigmoid(Z2)\n\ny_pred = (A2 &gt; 0.5).astype(int)\naccuracy = np.mean(y_pred == y_test)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")\n</code></pre> <pre><code>Test Accuracy: 0.9450\n</code></pre><p></p> <p>Finally, we can create corresponding visualization: </p><pre><code># Plot training loss\nplt.plot(train_losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n# Decision boundary visualization\nxx, yy = np.meshgrid(\n    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200)\n)\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\nZ1 = grid.dot(W1.T) + b1.T\nA1 = sigmoid(Z1)\nZ2 = A1.dot(W2.T) + b2.T\nA2 = sigmoid(Z2)\npreds = (A2 &gt; 0.5).astype(int).reshape(xx.shape)\n\nplt.contourf(xx, yy, preds, alpha=0.3, cmap=plt.cm.Paired)\nplt.scatter(X_test[:,0], X_test[:,1], c=y_test.ravel(), edgecolor='k', cmap=plt.cm.Paired)\nplt.title(\"Decision Boundary (Test Set)\")\nplt.show()\n</code></pre> This is the graph generated by plotting the loss after every epoch of training: <p></p> <p>And this is the plot with the decision boundary created: </p>"},{"location":"exercise3/main/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP","text":"<p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol> <p>To adjust to the multiple classes we need to make changes to the activation function, loss function and even add one-hot encoding to the data, the auxiliary functions are as follows: </p><pre><code># Softmax activation\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\n# Cross-entropy loss\ndef cross_entropy(y, y_hat):\n    eps = 1e-9\n    return -np.mean(np.sum(y * np.log(y_hat + eps), axis=1))\n\n# Derivative of cross-entropy wrt logits (softmax combined)\ndef cross_entropy_derivative(y, y_hat):\n    return (y_hat - y)\n\n# One-hot encoding\ndef one_hot(y, num_classes):\n    one_hot_matrix = np.zeros((y.size, num_classes))\n    one_hot_matrix[np.arange(y.size), y] = 1\n    return one_hot_matrix\n</code></pre><p></p> <p>The new data for this exercise can be made with the following code: </p><pre><code># Generate class 0 with 2 clusters\nX0, y0 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=2, n_classes=3, weights=[1.0, 0.0, 0.0],\n    random_state=42\n)\n\n# Generate class 1 with 3 clusters\nX1, y1 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=3, n_classes=3, weights=[0.0, 1.0, 0.0],\n    random_state=24\n)\n\n# Generate class 2 with 4 clusters\nX2, y2 = make_classification(\n    n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n    n_clusters_per_class=4, n_classes=3, weights=[0.0, 0.0, 1.0],\n    random_state=84\n)\n\n# Combine\nX = np.vstack((X0, X1, X2))\ny = np.hstack((y0, y1, y2))\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Convert labels to one-hot\nnum_classes = 3\ny_train_oh = one_hot(y_train, num_classes)\ny_test_oh = one_hot(y_test, num_classes)\n</code></pre><p></p> <p>The training code itself is similar to the previous exercise, with the use of softmax and change from binary cross-entropy to categorical cross-entropy: </p><pre><code># Network architecture\ninput_dim = 4\nhidden_dim = 8\noutput_dim = 3\n\n# Initialize parameters\nnp.random.seed(1)\nW1 = np.random.randn(hidden_dim, input_dim) * 0.01\nb1 = np.zeros((hidden_dim, 1))\nW2 = np.random.randn(output_dim, hidden_dim) * 0.01\nb2 = np.zeros((output_dim, 1))\n\n# Training setup\neta = 0.3\nepochs = 300\ntrain_losses = []\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward Pass\n    Z1 = X_train.dot(W1.T) + b1.T\n    A1 = np.tanh(Z1)\n\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = softmax(Z2)\n\n    # Loss Calculation\n    loss = cross_entropy(y_train_oh, A2)\n    train_losses.append(loss)\n\n    # Backward Pass\n    dZ2 = cross_entropy_derivative(y_train_oh, A2)\n    dW2 = dZ2.T.dot(A1)\n    db2 = np.sum(dZ2, axis=0, keepdims=True)\n\n    dA1 = dZ2.dot(W2)\n    dZ1 = dA1 * (1 - np.tanh(Z1) ** 2)\n    dW1 = dZ1.T.dot(X_train)\n    db1 = np.sum(dZ1, axis=0, keepdims=True)\n\n    # Normalize gradients\n    dW2 /= X_train.shape[0]\n    db2 /= X_train.shape[0]\n    dW1 /= X_train.shape[0]\n    db1 /= X_train.shape[0]\n\n    # Parameter Update\n    W1 -= eta * dW1\n    b1 -= eta * db1.T\n    W2 -= eta * dW2\n    b2 -= eta * db2.T\n\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> The same is true for the evaluation: <pre><code># Evaluation\nZ1 = X_test.dot(W1.T) + b1.T\nA1 = np.tanh(Z1)\nZ2 = A1.dot(W2.T) + b2.T\nA2 = softmax(Z2)\n\ny_pred = np.argmax(A2, axis=1)\naccuracy = np.mean(y_pred == y_test)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")\n</code></pre> <pre><code>Test Accuracy: 0.6567\n</code></pre><p></p> <p>And finally, this is the plot of the training loss: </p>"},{"location":"exercise3/main/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4: Multi-Class Classification with Deeper MLP","text":"<p>Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p> <p>To account for a new hidden layer, we made the hidden layer with W2, b2 and subsequently made W3, b3 for the output layer. Due to really low accuracy, scaling by 0.01 on the initialization was also changed. This is the resulting code:</p> <p></p><pre><code># Initialize parameters\nnp.random.seed(1)\nb1 = np.zeros((hidden1_dim, 1))\nb2 = np.zeros((hidden2_dim, 1))\nb3 = np.zeros((output_dim, 1))\n\nW1 = np.random.randn(hidden1_dim, input_dim) / np.sqrt(input_dim)\nW2 = np.random.randn(hidden2_dim, hidden1_dim) / np.sqrt(hidden1_dim)\nW3 = np.random.randn(output_dim, hidden2_dim) / np.sqrt(hidden2_dim)\n\n# Training setup\neta = 0.3\nepochs = 300\ntrain_losses = []\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward Pass\n    Z1 = X_train.dot(W1.T) + b1.T\n    A1 = np.tanh(Z1)\n\n    Z2 = A1.dot(W2.T) + b2.T\n    A2 = np.tanh(Z2)\n\n    Z3 = A2.dot(W3.T) + b3.T\n    A3 = softmax(Z3)\n\n    # Loss Calculation\n    loss = cross_entropy(y_train_oh, A3)\n    train_losses.append(loss)\n\n    # Backward Pass\n    dZ3 = cross_entropy_derivative(y_train_oh, A3)\n    dW3 = dZ3.T.dot(A2)\n    db3 = np.sum(dZ3, axis=0, keepdims=True)\n\n    dA2 = dZ3.dot(W3)\n    dZ2 = dA2 * (1 - np.tanh(Z2) ** 2)\n    dW2 = dZ2.T.dot(A1)\n    db2 = np.sum(dZ2, axis=0, keepdims=True)\n\n    dA1 = dZ2.dot(W2)\n    dZ1 = dA1 * (1 - np.tanh(Z1) ** 2)\n    dW1 = dZ1.T.dot(X_train)\n    db1 = np.sum(dZ1, axis=0, keepdims=True)\n\n    # Normalize gradients\n    dW3 /= X_train.shape[0]\n    db3 /= X_train.shape[0]\n    dW2 /= X_train.shape[0]\n    db2 /= X_train.shape[0]\n    dW1 /= X_train.shape[0]\n    db1 /= X_train.shape[0]\n\n    # Parameter Update\n    W1 -= eta * dW1\n    b1 -= eta * db1.T\n    W2 -= eta * dW2\n    b2 -= eta * db2.T\n    W3 -= eta * dW3\n    b3 -= eta * db3.T\n\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Evaluation\nZ1 = X_test.dot(W1.T) + b1.T\nA1 = np.tanh(Z1)\nZ2 = A1.dot(W2.T) + b2.T\nA2 = np.tanh(Z2)\nZ3 = A2.dot(W3.T) + b3.T\nA3 = softmax(Z3)\n\ny_pred = np.argmax(A3, axis=1)\naccuracy = np.mean(y_pred == y_test)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")\n</code></pre> <pre><code>Test Accuracy: 0.6833\n</code></pre><p></p> <p>The following is the resulting loss graph: </p>"},{"location":"exercise3/main/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>The deliverable for this activity consists of a report that includes:</p> <p>Important Notes:</p> <p>Usage of Toolboxes</p> <p>You may use toolboxes (e.g., NumPy) ONLY for matrix operations and calculations during this activity. All other computations, including activation functions, loss calculations, gradients, and the forward pass, MUST BE IMPLEMENTED within your MLP (Multi-Layer Perceptron) code. The use of third-party libraries for the MLP implementation IS STRICTLY PROHIBITED.</p> <p>Failure to comply with these instructions will result in your submission being rejected.</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> <ul> <li> <p>Exercise 1 (2 points):</p> <ul> <li>Forward pass fully explicit (0.5 points)</li> <li>Loss and backward pass with all gradients derived (1 point)</li> <li>Parameter updates shown correctly (0.5 point)</li> <li>Deductions for missing steps or incorrect math.</li> </ul> </li> <li> <p>Exercise 2 (3 points):</p> <ul> <li>Correct data generation and splitting (0.5 points)</li> <li>Functional MLP implementation from scratch (2 point)</li> <li>Training, evaluation, and results reported (0.5 points)</li> <li>Deductions for using forbidden libraries in the model core or poor performance due to errors.</li> </ul> </li> <li> <p>Exercise 3 (2 points + 1 extra):</p> <ul> <li>Correct data generation and splitting (0.5 points)</li> <li>Functional MLP for multi-class (1.5 points)</li> <li>Training, evaluation, and results (1 point)</li> <li>Extra point: Exact reuse of Exercise 2's MLP code structure (1 point, optional)</li> <li>Deductions similar to Exercise 2; extra point only if reuse is verbatim in core logic.</li> </ul> </li> <li> <p>Exercise 4 (2 points):</p> <ul> <li>Successful adaptation of Exercise 3 with at least 2 hidden layers (1 point)</li> <li>Training and evaluation results showing functionality (1 point)</li> <li>Deductions if architecture doesn't meet the depth requirement or if results are not provided.</li> </ul> </li> </ul> <p>Overall: Submissions must be clear, well-documented (code comments, explanations), and reproducible.</p>"},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}