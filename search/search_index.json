{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#eric-possato","title":"Eric Possato","text":"<p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercise1/main/","title":"Exercise 1","text":"<p>Deadline and Submission</p> <p> 05.sep (friday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Data Preparation and Analysis for Neural Networks</p> <p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"},{"location":"exercise1/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercise1/main/#exploring-class-separability-in-2d","title":"Exploring Class Separability in 2D","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"exercise1/main/#instructions","title":"Instructions","text":"<ol> <li>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:<ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> </li> <li> <p>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable. </p><pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Number of samples per class\nn_samples = 100  \n\n# Class parameters: (mean, std)\nclass_params = {\n    0: {\"mean\": [2, 3], \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6], \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1], \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]}\n}\n\n# Store all samples\ndata = []\nlabels = []\n\nfor label, params in class_params.items():\n    mean = params[\"mean\"]\n    std = params[\"std\"]\n\n    # Generate Gaussian distributed samples\n    x = np.random.normal(mean[0], std[0], n_samples)\n    y = np.random.normal(mean[1], std[1], n_samples)\n\n    # Stack into dataset\n    samples = np.column_stack((x, y))\n    data.append(samples)\n    labels.extend([label] * n_samples)\n\n# Combine into full dataset\ndata = np.vstack(data)\nlabels = np.array(labels)\n\n# Put into a DataFrame\ndf = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\ndf[\"class\"] = labels\n\n# Visualization\nplt.figure(figsize=(8,6))\nfor label in class_params.keys():\n    subset = df[df[\"class\"] == label]\n    plt.scatter(subset[\"x1\"], subset[\"x2\"], label=f\"Class {label}\", alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Synthetic Gaussian Dataset (4 classes)\")\n# Save image\nplt.savefig(\"synthetic_gaussian_dataset.png\")\n</code></pre><p></p> <p></p> </li> <li> <p>Analyze and Draw Boundaries:</p> <ol> <li> <p>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</p> <ul> <li> <p>Class 0 has a spread across most of the vertical axis and and is between 0.0 and 4.0 horizontally. Slightly overlaps Class 1</p> </li> <li> <p>Class 0 has a diagonal spread, between 2 and 10 vertically as well as horizontally. Slightly overlaps Class 0 in a few points and Class 2 in fewer points.</p> </li> <li> <p>Class 2 is very concentrated and relatvely isolated from other classes.</p> </li> <li> <p>Class 3 is completely isolated, spread vertically along and the furthest in the x axis.</p> </li> </ul> </li> <li> <p>Based on your visual inspection, could a simple, linear boundary separate all classes?</p> <ul> <li>A linear boundary is capable of mostly separating all classes (with some missclassified points), but because of the overlaps between class 0 and 1, a perfect and clean linear separation would fail.</li> </ul> </li> <li> <p>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</p> <ul> <li>By adding the following code to the earlier plot, we can approximate some separations between classes <pre><code># three hand-drawn separators \nplt.plot([6.0, 0.6], [-2.0, 10.0], linestyle=\"--\", linewidth=2, color=\"k\", label=\"approx. boundary\")  # between 0 &amp; 1\nplt.plot([4.0, 12.0], [2.0, 5.0], linestyle=\"--\", linewidth=2, color=\"k\")                            # between 1 &amp; 2\nplt.plot([12.0, 12.0], [-1, 9], linestyle=\"--\", linewidth=2, color=\"k\")                             # isolate class 3\n</code></pre> </li> </ul> </li> </ol> </li> </ol>"},{"location":"exercise1/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercise1/main/#non-linearity-in-higher-dimensions","title":"Non-Linearity in Higher Dimensions","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"exercise1/main/#instructions_1","title":"Instructions","text":"<ol> <li> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p> <ul> <li> <p>Class A:</p> <p>Mean vector:</p> \\[\\mu_A = [0, 0, 0, 0, 0]\\] <p>Covariance matrix:</p> \\[ \\Sigma_A = \\begin{pmatrix} 1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\ 0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\ 0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0 \\end{pmatrix} \\] </li> <li> <p>Class B:</p> <p>Mean vector:</p> \\[\\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\\] <p>Covariance matrix:</p> \\[ \\Sigma_B = \\begin{pmatrix} 1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\ -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\ 0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5 \\end{pmatrix} \\] </li> </ul> </li> <li> <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).     <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# set experimen seed\nnp.random.seed(42)\n\n# number of samples\nn_a = 500\nn_b = 500\n\n# Parameters for Class A\nmu_A = np.array([0, 0, 0, 0, 0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\n# Parameters for Class B\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2,  0.4, 1.5, 0.6, 0.0],\n    [0.0,  0.0, 0.6, 1.5, 0.3],\n    [0.0,  0.0, 0.0, 0.3, 1.5],\n])\n\n# Generate Data\nA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=n_a)\nB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=n_b)\n\n# Combine into one dataset\nX = np.vstack([A, B])\ny = np.array([0] * n_a + [1] * n_b)  # 0 = Class A, 1 = Class B\n\n# Put into DataFrame\ndf = pd.DataFrame(X, columns=[f\"x{i+1}\" for i in range(5)])\ndf[\"class\"] = y\n\n\n# Standardize features\nscaler = StandardScaler()\nX_std = scaler.fit_transform(df[[f\"x{i+1}\" for i in range(5)]])\n\n# PCA projection to 2D\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_std)\nevr = pca.explained_variance_ratio_ \n\n# ---------- Plot ----------\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], alpha=0.6, label=\"Class A\")\nplt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], alpha=0.6, label=\"Class B\")\nplt.xlabel(f\"PC1 ({evr[0]*100:.1f}% var)\")\nplt.ylabel(f\"PC2 ({evr[1]*100:.1f}% var)\")\nplt.title(\"Exercise 2: PCA projection to 2D (Class A vs Class B)\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"exercise2_pca_scatter.png\", dpi=150, bbox_inches=\"tight\")\nprint(\"Saved PCA figure to exercise2_pca_scatter.png\")\nplt.show()\n</code></pre> </li> </ul> </li> <li> <p>Analyze the Plots:</p> <ol> <li>Based on your 2D projection, describe the relationship between the two classes.<ul> <li>Both classes have similar vertical spreads. Horizontally, class A is concentrated mostly on negative values, and Class B on positive values. They both overlap between -2 and 2 within PC1 axis</li> </ul> </li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.<ul> <li>Linear models use a hyperplane to separate the data, but when classes are intertwined like shown in the image, this fails, since a hyperplane capable of separating the classes doesn't exist. </li> </ul> </li> </ol> </li> </ol>"},{"location":"exercise1/main/#exercise-3","title":"Exercise 3","text":""},{"location":"exercise1/main/#preparing-real-world-data-for-a-neural-network","title":"Preparing Real-World Data for a Neural Network","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (<code>tanh</code>) activation function in its hidden layers.</p>"},{"location":"exercise1/main/#instructions_2","title":"Instructions","text":"<ol> <li>Get the Data: Download the Spaceship Titanic dataset from Kaggle.</li> <li>Describe the Data:<ul> <li>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).<ul> <li>The dataset comes from the Kaggle competition: Spaceship Titanic. The dataset consists of records recovered from the spaceship's damaged computer system after the ship collided with an anomaly. The objective of the competition and consequently, the data, is to be able to predict which passengers were transported to an alternate dimension by the anomaly</li> </ul> </li> <li>List the features and identify which are numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>). <pre><code>import pandas as pd\n\n# Load dataset (adjust path to your train.csv)\ndf = pd.read_csv(\"./docs/exercise1/titanic_dataset/train.csv\")\n\n\n# List all columns\nprint(\"\\nColumns in dataset:\")\nprint(df.columns.tolist())\n\n# Show data types and non-null counts\nprint(\"\\nInfo about dataset:\")\nprint(df.info())\n\n\n# Quick separation into numerical vs categorical\nnumerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_cols = df.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n\nprint(\"\\nNumerical features:\", numerical_cols)\nprint(\"Categorical features:\", categorical_cols)\n</code></pre> <pre><code>Numerical features: ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nCategorical features: ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name', 'Transported']\n</code></pre></li> <li>Investigate the dataset for missing values. Which columns have them, and how many? <pre><code># Summary of missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isna().sum())\n</code></pre> <pre><code>Missing values per column:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\ndtype: int64\n</code></pre></li> </ul> </li> <li>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.<ul> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ul> </li> <li>Visualize the Results:<ul> <li>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) before and after scaling to show the effect of your transformation.</li> </ul> </li> </ol> <p>Methods for 3 and 4:</p> <ul> <li>Numerical features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) were imputed with the median value of each column. Using median instead of mean makes it less likely that outliers influence the results.</li> <li>Categorical features (HomePlanet, CryoSleep, Destination, VIP) were imputed with the mode (most frequent value). THis preserves the most common class for each feature</li> <li>Dropped columns: Cabin and Name were removed. Name does not offer any useful information and Cabin is very complex, not usable without heavy feature engineering, so dropped for simplicity.</li> <li>Binary variables (CryoSleep, VIP) were converted into 0/1 integers. This allows direct use as inputs in a neural network.</li> <li>Nominal categorical variables (HomePlanet, Destination) were transformed using one-hot encoding. This removes ordinal bias by making each category a separate feature.</li> <li>All numerical features were scaled to [-1, 1] using Min\u2013Max normalization. The tanh activation function outputs values in the range [-1, 1] so this process scales the inputs to the required range.</li> </ul> <p>This is the code that combines all of the mentioned methos and generates histograms for the <code>Age</code> and <code>FoodCourt</code> columns: </p><pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_csv(\"./docs/exercise1/titanic_dataset/train.csv\")\n\n# Handle Missing Data\n\n# Numerical: fill with median\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    df[col] = df[col].fillna(df[col].median())\n\n# Categorical: fill with mode\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\nfor col in cat_cols:\n    # make column string dtype\n    df[col] = df[col].astype(\"string\")\n    # fill NaN with mode and convert to string dtype\n    df[col] = df[col].fillna(df[col].mode(dropna=True)[0]).astype(str)\n\n# Drop Cabin and Name\ndf.drop([\"Cabin\", \"Name\"], axis=1, inplace=True)\n\n# Convert CryoSleep and VIP from True/False strings to 0/1 ints\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].map({\"True\": 1, \"False\": 0})\ndf[\"VIP\"] = df[\"VIP\"].map({\"True\": 1, \"False\": 0})\n\n# Encode Categorical\ndf[\"CryoSleep\"] = df[\"CryoSleep\"].astype(int)\ndf[\"VIP\"] = df[\"VIP\"].astype(int)\n\n# One-hot encode categorical vars\ndf = pd.get_dummies(df, columns=[\"HomePlanet\", \"Destination\"], drop_first=True)\n\n# Capture copy BEFORE scaling\npre_scale_num = df[num_cols].copy()\n\n# Scale Numerical Features\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndf[num_cols] = scaler.fit_transform(df[num_cols].astype(\"float64\"))\n\nprint(df.head())\n\n# histograms before vs after\nfeatures_to_show = [\"Age\", \"FoodCourt\"]  # pick any subset of num_cols\n\nfor col in features_to_show:\n    if col in pre_scale_num.columns:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=False, sharey=False)\n        ax[0].hist(pre_scale_num[col].dropna(), bins=30)\n        ax[0].set_title(f\"{col} \u2014 before scaling\")\n        ax[0].set_xlabel(col); ax[0].set_ylabel(\"count\")\n\n        ax[1].hist(df[col].dropna(), bins=30)\n        ax[1].set_title(f\"{col} \u2014 after scaling (standardized)\")\n        ax[1].set_xlabel(col); ax[1].set_ylabel(\"count\")\n\n        plt.tight_layout()\n        # Save figures\n        plt.savefig(f\"hist_{col.lower()}_before_after.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n</code></pre><p></p> <p>This are the histograms generated by this code:</p> <p> </p>"},{"location":"exercise1/main/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>The deliverable for this activity consists of a report that includes:</p> <ol> <li>A brief description of your approach to each exercise.</li> <li>The code used to generate the datasets, preprocess the data, and create the visualizations. With comments explaining each step.</li> <li>The plots and visualizations requested in each exercise.</li> <li>Your analysis and answers to the questions posed in each exercise.</li> </ol> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> <p>Exercise 1 (3 points):</p> Criteria Description 1 pt Data is generated correctly and visualized in a clear scatter plot with proper labels and colors. 2 pts The analysis of class separability is accurate, and the proposed decision boundaries are logical and well-explained in the context of what a network would learn. <p>Exercise 2 (3 points):</p> Criteria Description 1 pt Data is generated correctly using the specified multivariate parameters. 1 pt Dimensionality reduction is applied correctly, and the resulting 2D projection is clearly plotted. 1 pt The analysis correctly identifies the non-linear relationship and explains why a neural network would be a suitable model. <p>Exercise 3 (4 points):</p> Criteria Description 1 pt The data is correctly loaded, and its characteristics are accurately described. 2 pts All preprocessing steps (handling missing data, encoding, and appropriate feature scaling for <code>tanh</code>) are implemented correctly and with clear justification for a neural network context. 1 pt Visualizations effectively demonstrate the impact of the data preprocessing."},{"location":"exercise2/main/","title":"Exercise 2","text":"<p>Deadline and Submission</p> <p> 14.sep (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Understanding Perceptrons and Their Limitations</p> <p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"},{"location":"exercise2/main/#exercise-1","title":"Exercise 1","text":""},{"location":"exercise2/main/#data-generation-task","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:  </p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([1.5, 1.5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\) (i.e., variance of \\(0.5\\) along each dimension, no covariance).  </p> </li> <li> <p>Class 1:</p> <p>Mean = \\([5, 5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\).  </p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p> <p>We can use the following code to generate and plot the points: </p><pre><code>def generate_and_plot():\n    # Parameters for Class 0\n    mean_class0 = [1.5, 1.5]\n    cov_class0 = [[0.5, 0], [0, 0.5]]\n\n    # Parameters for Class 1\n    mean_class1 = [5, 5]\n    cov_class1 = [[0.5, 0], [0, 0.5]]\n\n    # Fix random seed for reproducibility\n    np.random.seed(42)\n\n    # Generate 1000 samples per class\n    class0_samples = np.random.multivariate_normal(mean_class0, cov_class0, 1000)\n    class1_samples = np.random.multivariate_normal(mean_class1, cov_class1, 1000)\n\n    # Plot the two classes\n    plt.figure(figsize=(8, 6))\n    plt.scatter(class0_samples[:, 0], class0_samples[:, 1], \n                c='blue', alpha=0.5, label=\"Class 0\")\n    plt.scatter(class1_samples[:, 0], class1_samples[:, 1], \n                c='red', alpha=0.5, label=\"Class 1\")\n\n    plt.title(\"2D Data Points from Two Classes (Multivariate Normal)\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid(True)\n    #plt.show()\n    # Save the image as a png file\n    plt.savefig(\"exercise2_ex1_plot.png\")\n    # Return values for reuse\n    return class0_samples, class1_samples\n</code></pre><p></p> <p>The following is the resulting plot of the classes: </p>"},{"location":"exercise2/main/#perceptron-implementation-task","title":"Perceptron Implementation Task:","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.  </p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).  </li> <li>Use the perceptron learning rule: For each misclassified sample \\((x, y)\\), update \\(w = w + \u03b7 * y * x\\) and \\(b = b + \u03b7 * y\\), where \\(\u03b7\\) is the learning rate (start with \\(\u03b7=0.01\\)).  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.  </li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by \\(w\u00b7x + b = 0\\)) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p> <p>The following code can be used to implment and train a perceptron:</p> <p></p><pre><code># Perceptron implementation\ndef perceptron_train(X, y, lr=0.01, max_epochs=100):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)   # weight vector\n    b = 0.0                    # bias term\n    accuracies = []\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for i in range(n_samples):\n            activation = np.dot(w, X[i]) + b\n            y_pred = 1 if activation &gt;= 0 else -1\n            if y_pred != y[i]:\n                # Update rule\n                w += lr * y[i] * X[i]\n                b += lr * y[i]\n                errors += 1\n\n        # Track accuracy\n        activation = np.dot(X, w) + b\n        predictions = np.where(activation &gt;= 0, 1, -1)\n        acc = np.mean(predictions == y)\n        accuracies.append(acc)\n\n        if errors == 0:  # converged\n            print(f\"Converged after {epoch+1} epochs.\")\n            break\n\n    return w, b, accuracies, predictions\n</code></pre> We can use that code, to train a perceptron based to the parameters specified by the exercise, and plot the relevant graphs: <pre><code># Generate data\nX, y, class0_samples, class1_samples = generate_data()\n\n# Train perceptron\nw, b, accuracies, final_predictions = perceptron_train(X, y)\n\n# Results\nprint(\"\\nFinal Results:\")\nprint(\"Weights:\", w)\nprint(\"Bias:\", b)\nprint(\"Final Accuracy:\", accuracies[-1])\n\n# Plot decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nxx = np.linspace(x_min, x_max, 100)\nyy = -(w[0] * xx + b) / w[1]  # line equation\n\nplt.figure(figsize=(8, 6))\nplt.scatter(class0_samples[:, 0], class0_samples[:, 1], c='blue', alpha=0.5, label=\"Class 0 (-1)\")\nplt.scatter(class1_samples[:, 0], class1_samples[:, 1], c='red', alpha=0.5, label=\"Class 1 (+1)\")\nplt.plot(xx, yy, 'k--', label=\"Decision Boundary\")\n\n# Misclassified points\nmisclassified = X[final_predictions != y]\nif len(misclassified) &gt; 0:\n    plt.scatter(misclassified[:, 0], misclassified[:, 1],\n                edgecolors='yellow', facecolors='none', s=80, label=\"Misclassified\")\n\nplt.title(\"Perceptron Decision Boundary\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot training accuracy\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(accuracies) + 1), accuracies, marker='o')\nplt.title(\"Training Accuracy over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0, 1.05)\nplt.grid(True)\nplt.show()\n</code></pre><p></p> <p>This is the resulting decision boundary: </p> <p>This is the training accuracy over each epoch: </p> <p>It is worth mentioning that since the data used is linearly separable, the perceptron is able to perfectly classify the points given.</p>"},{"location":"exercise2/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercise2/main/#data-generation-task_1","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([3, 3]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\) (i.e., higher variance of 1.5 along each dimension).</p> </li> <li> <p>Class 1:</p> <p>Mean = \\([4, 4]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\).  </p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p> <p>Using the same method as the first exercise, while changing the parameters, we get the following data:  Notice how in this case, both classes overlap in a number of areas.</p>"},{"location":"exercise2/main/#perceptron-implementation-task_1","title":"Perceptron Implementation Task:","text":"<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.  </p> <ul> <li>Follow the same initialization, update rule, and training process.  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.  </li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p> <p>Since this case is not linearly separable, and the training may vary in result, we can add random initialization of the weights and bias, this is the new perceptron training function to account for that: </p><pre><code># Perceptron implementation\ndef perceptron_train(X, y, lr=0.01, max_epochs=100, random_init=False):\n    n_samples, n_features = X.shape\n    # Initialize weights randomly or zeros\n    if random_init:\n        w = np.random.randn(n_features) * 0.01\n        b = np.random.randn() * 0.01\n    else:\n        w = np.zeros(n_features)\n        b = 0.0           # bias term\n    accuracies = []\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for i in range(n_samples):\n            activation = np.dot(w, X[i]) + b\n            y_pred = 1 if activation &gt;= 0 else -1\n            if y_pred != y[i]:\n                # Update rule\n                w += lr * y[i] * X[i]\n                b += lr * y[i]\n                errors += 1\n\n        # Track accuracy\n        activation = np.dot(X, w) + b\n        predictions = np.where(activation &gt;= 0, 1, -1)\n        acc = np.mean(predictions == y)\n        accuracies.append(acc)\n\n        if errors == 0:  # converged\n            print(f\"Converged after {epoch+1} epochs.\")\n            break \n    return w, b, accuracies, predictions\n</code></pre><p></p> <p>Accounting for this change, this is reflected in the main code as follows: </p><pre><code># Generate data\nX, y, class0_samples, class1_samples = generate_data()\n\n# Train perceptron\nall_runs_acc = []\nbest_run = None\n\nfor run in range(5):\n    w, b, accuracies, final_predictions = perceptron_train(X, y, lr=0.01, max_epochs=100, random_init=True)\n    final_acc = accuracies[-1]\n    all_runs_acc.append(final_acc)\n\n    if best_run is None or final_acc &gt; best_run[\"acc\"]:\n        best_run = {\"w\": w, \"b\": b, \"acc\": final_acc, \"acc_list\": accuracies, \"pred\": final_predictions}\n\n\n# Results\nprint(\"\\nSummary over 5 runs:\")\nprint(\"Accuracies per run:\", all_runs_acc)\nprint(\"Average accuracy:\", np.mean(all_runs_acc))\nprint(\"Best accuracy:\", best_run[\"acc\"])\nprint(\"Best weights:\", best_run[\"w\"])\nprint(\"Best bias:\", best_run[\"b\"])\n\n\n\n# Plot decision boundary of best run\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nxx = np.linspace(x_min, x_max, 100)\nyy = -(best_run[\"w\"][0] * xx + best_run[\"b\"]) / best_run[\"w\"][1]\n\nplt.figure(figsize=(8, 6))\nplt.scatter(class0_samples[:, 0], class0_samples[:, 1], \n            c='blue', alpha=0.5, label=\"Class 0 (-1)\")\nplt.scatter(class1_samples[:, 0], class1_samples[:, 1], \n            c='red', alpha=0.5, label=\"Class 1 (+1)\")\nplt.plot(xx, yy, 'k--', label=\"Decision Boundary\")\n\n# Misclassified points separated by class\nmisclassified = X[best_run[\"pred\"] != y]\nmisclassified_labels = y[best_run[\"pred\"] != y]\n\n# Class 0 misclassified (true label = -1)\nplt.scatter(misclassified[misclassified_labels == -1][:, 0],\n            misclassified[misclassified_labels == -1][:, 1],\n            c='green', alpha=0.5, label=\"Misclassified Class 0\")\n\n# Class 1 misclassified (true label = +1)\nplt.scatter(misclassified[misclassified_labels == 1][:, 0],\n            misclassified[misclassified_labels == 1][:, 1],\n            c='yellow', alpha=0.5, label=\"Misclassified Class 1\")\n\nplt.title(\"Best Perceptron Decision Boundary\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.legend()\nplt.grid(True)\n\nplt.savefig(\"exercise2_ex2_perceptron_accuracy.png\")\n</code></pre><p></p> <p>This is the decision boundary generated by this code, with highlighted missclassified points in different colors: </p> <p>Due to the classes being closer together, even overlapping in a few situations, a perceptron has difficulty in finding a good separation of the data, in the case shown by the image, the perceptron seemed to favor guessing Class 1, missclassifying most of Class 0 points.</p> <p>This can be reinforced by analyzing the accuracy throughout the epochs, that remained consistently around 50%, showing the difficulty that the perceptron while performing with non linearly separable data.</p> <p></p>"},{"location":"exercise2/main/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Usage of Toolboxes</p> <p>You may use toolboxes (e.g., NumPy) ONLY for matrix operations and calculations during this activity. All other computations, including activation functions, loss calculations, gradients, and the forward pass, MUST BE IMPLEMENTED within your Perceptron code. The use of third-party libraries for the Perceptron implementation IS STRICTLY PROHIBITED.</p> <p>Failure to comply with these instructions will result in your submission being rejected.</p> <p>The deliverable for this activity consists of a report that includes:</p> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> Criteria Description 4 pts Correctness of the perceptron implementation 2 pts Exercise 1: Data generation, training, evaluation, and analysis. 2 pts Exercise 2: Data generation, training, evaluation, and analysis. 1 pt Visualizations: Quality and clarity of plots (data distribution, decision boundary, accuracy over epochs). 1 pt Report Quality: Clarity, organization, and completeness of the report."},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}